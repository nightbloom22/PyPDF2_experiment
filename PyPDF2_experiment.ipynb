{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3366823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader, PdfFileWriter, PdfFileMerger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71c1c8",
   "metadata": {},
   "source": [
    "### Extracting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64426048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/Author': '', '/CreationDate': \"D:20171004123236+08'00'\", '/Creator': 'PScript5.dll Version 5.2.2', '/ModDate': \"D:20171004123236+08'00'\", '/Producer': 'Acrobat Distiller 11.0 (Windows)', '/Title': ''}\n"
     ]
    }
   ],
   "source": [
    "filename = 'paper1.pdf'\n",
    "with open(filename, 'rb') as f:\n",
    "    pdf = PdfFileReader(f)\n",
    "    information = pdf.getDocumentInfo()\n",
    "    number_of_pages = pdf.getNumPages()\n",
    "    print(information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19f36b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: \n",
      "Creator: PScript5.dll Version 5.2.2\n",
      "Producer: Acrobat Distiller 11.0 (Windows)\n",
      "Number of pages: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"Author\" +': ' + information.author)\n",
    "print(\"Creator\" +': ' + information.creator)\n",
    "print(\"Producer\" +': ' + information.producer)\n",
    "print(\"Number of pages:\", number_of_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dcbc75",
   "metadata": {},
   "source": [
    "### Extracting text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6668f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "pdfObject = open(filename, 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "805de595",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for i in range(0, pdfReader.numPages):\n",
    "    pageObj = pdfReader.getPage(i)\n",
    "    text = text + pageObj.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "38c425d8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Algorithms 2017, 10, 114; doi:10.3390/a10040114  www.mdpi.com/journal/algorithms \n",
      "Article \n",
      "Variable Selection in Time Series Forecasting Using Random Forests \n",
      "Hristos Tyralis * and Georgia Papacharalampous \n",
      "Department of Water Resources and Environmental Engi neering, School of Civil Engineering, National Technical University of Athens, Iroon Polytechniou 5, 157 80 Zografou, Greece; papacharalampous.georgia@gmail.com * Correspondence: montchrister@gmail.com; Tel.: +30-21-0514-8526 Received: 29 July 2017; Accepted: 1 Octo ber 2017; Published: 4 October 2017 \n",
      "Abstract: Time series forecasting using machine learning algorithms has gained popularity recently. Random forest is a machine learning algorithm im plemented in time series forecasting; however, most of its forecasting properties have remain ed unexplored. Here we focus on assessing the performance of random forests in one-step forecast ing using two large datasets of short time series with the aim to suggest an optimal set of pred ictor variables. Furthermore, we compare its performance to benchmarking methods. The first dataset is composed by 16,000 simulated time series from a variety of Autoregressive Fractionally Integrated Moving Average (ARFIMA) models. The second dataset consists of 135 mean annual temperature time series. The highest predictive performance of RF is observed when using a low nu mber of recent lagged predictor variables. This outcome could be useful in relevant future applications, with the prospect to achieve higher predictive accuracy. Keywords: ARFIMA; ARMA; machine learning; one-step ah ead forecasting; random forests; time series forecasting; variable selection  \n",
      "1. Introduction 1.1. Time Series Forecasting and Random Forests The use of machine learning algorithms in predic tive modelling (see for the definition [1]) and in the specific case of time series forecasting has increased considerably recently [2]. Relevant applications can be found in the review papers [3–6]. Time series forecasting can be classified into tw o categories according to the forecast horizon, i.e., one-step and multi-step forecasting [2,7]. The one-step forecasting (examined here) is useful in single applications, e.g., in assessing forecasting methods using simulations [8], engineering forecasting [9,10], environmental forecasting [11,12] , and financial forecasting [13–17]. Furthermore, the recursive and the DiRec multi-step forecasting st rategies depend on the one-step forecasting. In particular, one-step ahead forecasts are used in the recursive strategy, while in each step the forecasted value of the previous step is used as additional input, without changing the forecasting model. The DiRec strategy combines the recursive strategy, but the forecasting model changes in every step [7]. The random forest (RF) is a machine learning algorithm introduced in [18], which can be used for classification and regression. It is popular becaus e it can be applied to a wide range of prediction problems, it has a few parameters to tune, it is simple to use, it has been successfully applied to many practical problems and it can deal with small sample sizes, high-dimensional feature spaces, and complex data structures [19,20]. A review and a simp le presentation of the RF algorithm can be found in [20–22]. Regression using RF can be implemented for time series forecasting purposes. Algorithms 2017, 10, 114  2 of 26 \n",
      "Representative applications can be found in many scientific fields including these of engineering [23,24], environmental and geophysical sciences [25–27], financial studies [28,29], and medicine [30], with varying performance. Furthermore, small datase ts are used in these applications; therefore, the results cannot be generalized. The performance of the RF algorithm depends on the tuning of its parameters and the variable selection (also known as feature selection). Procedur es for estimating the optimal set of parameters are proposed in several studies. These procedur es consider the performance of the algorithm depending on the selected variables [22,31], the nu mber of trees [32,33], the number of possible directions for splitting at each node of each tree [20,34] (p. 199), and the number of examples in each cell, below which the cell is not split [20,35]. Ther e are also some studies examining the RF properties in a theoretical context [19]. The performance of machine learning algorithms is usually assessed using single case studies. Sometimes large datasets composed of real-world studies are used for benchmarking and the comparison of several algorithms [7,36–40]. More over, [41] proposes the use of simulations as a considerable alternative. Respective applications can be found in [42,43]. 1.2. A Framework to Assess the Performance of Random Forests in Time Series Forecasting In usual regression problems, a sample consisting  of observations of the dependent variable and the respective observations of the predictor variable s is given. The regression model is trained using this sample. Prediction is then performed when new observations of the predictor variables are obtained. If we decide to use fewer predictor variable s than that included in the sample, then the size of the training set does not change. Moreover, it is important that the inclusion of unimportant predictor variables does not seriously impact the pr edictive performance of RF, as indicated in [34] (p. 489). On the other hand, using RF for time series forecasting is not identical to the simple regression case. In this case, the role of the predicto r variables is taken by previous values of the time series (lagged variables). Therefore, increasing the number of predictor variables, i.e., the selected lagged variables, inevitably results in reducing the length of the training set. Using fewer predictor variables instead may reduce the information obtain ed by the available knowledge of the temporal dependence. The analytical computation of the performance is not attainable, due to the complexity of the RF algorithm [20]. Nevertheless, an empirical answer co uld be given in the context of a large experiment. Furthermore, in this simulation experiment the RF could be compared to benchmarking methods, whose performance is known a priori, theoretically.  Here we perform a large simulation experiment, in which we use 16,000 simulated time series from a variety of Autoregressive Fractionally Integrated Moving Average (ARFIMA) models [44,45]. Similar studies using simulations from the family of ARFIMA models can be found in [46,47], although  to a lesser extent and implementing different methods. Simulation studies are effectively comp lemented when integrated with case studies; consequently, we additionally apply the methods to a dataset of 135 mean annual instrumental temperatures from the dataset in [48]. The length of  each simulated and observed time series is 101. We compare the performance of five algorithms in forecasting the 101st value of each time series, when fitted to its first 100 values. The five al gorithms implemented in the study are two naïve methods, an ARFIMA model, the theta method and the RF algorithm. The naïve methods usually perform well in predicting time series [37] and the ARFIMA processes are traditional methods which are frequently used for time series forecasting [ 45]. The theta method has been recently introduced and is also one of the most successful forecastin g methods [49]. We use various versions of the RF algorithm with respect to the number of predictor variables, while the optimization of the parameter set is performed using methods proposed in [50, 51]. The metrics used for the comparison are the mean absolute error (MAE), the mean squared error (MSE), the mean absolute percentage error (MAPE), and the linear regression coefficient. Th e analysis performed here regards time series simulated from stationary stochastic processes, wh ile stationary models are also used to model the temperature time series. Algorithms 2017, 10, 114  3 of 26 \n",
      "1.3. Aim of the Study The primary aim of our study is to investigate how the performance of RF is related to the variable selection in one-step forecasting of short time series. The proposed framework in Section 1.2 can help in providing an empirical answer to the problem of variable selection. In conclusion, it is shown that a low number of recent lagged variable s performs better, highlighting the importance of the training set’s length. The RF algorithm is proven to be competent in time series forecasting, even though it has not exhibited the best performance in most of the examined cases. This does not imply that the RF are of little practical value. Instead, as  has been shown in [12], there is not a universally best forecasting method, therefore, in practical applications we should use multiple methods for obtaining a good forecast. In such cases, we emphasize that appropriate methodologies for the optimization of the performance of each method should be used. Our contribution regards the optimization of the forecasting performance of the RF. 2. Methods and Data  In Section 2, we present the methods and data used in the present study. In particular, we present the definitions of the models used for simulation and testing, as well as the forecasting methods. Additionally, we present the temperature data, also  used for the testing of the methods. The codes and data to reproduce the study are available as supplementary material (see Appendix A). The computations were performed using the R programming language [52]. 2.1. Methods 2.1.1. Definition of ARMA and ARFIMA Models Here, we provide the definitions of the autore gressive moving average (ARMA) and ARFIMA stochastic processes, while the reader is also referred to [45] (pp. 6–65, 489–494) for further information. A time series in discrete time is defined as a sequence of observations x\n",
      "1, x2, …, of a certain phenomenon, while the time t is stated as a subscript to each value x\n",
      "t. A time series can be modelled by a stochastic process. The latter is a sequence of random variables x\n",
      "1, x2, …. Random \n",
      "variables are underlined according to the notation used in [53]. Let us consider a stationary stochastic process of normally-distributed random variables. The mean μ of the stochastic process is defined by: μ := E[x\n",
      "t], (1) The standard deviation function σ of the stochastic process is defined by: σ := (Var[x\n",
      "t])0.5 (2) The covariance function between x\n",
      "t and x t+k, γk of the stochastic process is defined by: \n",
      "γk := E[(x t − μ)(x t+k − μ)] (3) The correlation function between x\n",
      "t and x t+k, ρk of the stochastic process is defined by: \n",
      "ρk := γ k/σ2 (4) A normal stationary stochastic process {a\n",
      "t} is called a white noise process, if it is a sequence of \n",
      "uncorrelated random variables. Let us consider hereinafter that the white noise is a variable with zero mean, unless mentioned otherwise, and standard deviation σ\n",
      "a. We define the stochastic process {y\n",
      "t} by: \n",
      "yt := x t − μ (5) Let us consider the operator Β, which is defined by: B\n",
      "jxt = x t−j (6) Then the operator φ\n",
      "p(B) is defined by: Algorithms 2017, 10, 114  4 of 26 \n",
      "φp(B) := (1 − φ 1B − … − φ pBp) (7) The stochastic process {x\n",
      "t} is an autoregressive AR(p) model, if: \n",
      "φp(B)y t = a t, (8) which can be written in the following form: y\n",
      "t = φ 1yt−1 + … + φ pyt−p + a t (9) Let us also consider the operator θ\n",
      "q(B), which is defined by: θ\n",
      "q(B) := (1 + θ 1B + … + θ qBq) (10) The stochastic process {x\n",
      "t} is a moving average MA(q) model, if: \n",
      "yt = θ q(B) a t, (11) which can be written in the following form: y\n",
      "t = a t + θ 1at−1 + … + θ qat−q (12) The stochastic process {x\n",
      "t} is an autoregressive moving average ARMA( p, q) model, if \n",
      "φp(B)y t = θ q(B)a t, (13) which can be written in the following form: y\n",
      "t = φ 1yt−1 + … + φ pyt−p + a t + θ 1at−1 + … + θ qat−q (14) Let d ∊ (−0.5, 0.5). The stochastic process {x\n",
      "t} is an ARFΙMA(p, d, q), if \n",
      "φp(B)(1 − B)dxt = θ q(B)a t (15)2.1.2. Simulation of ARMA and ARFIMA Models We simulate the ARMA and ARFIMA processes usin g the arima.sim built in R function [52] and the fracdiff.sim algorithm of the R package fracdiff [54], respectively. 2.1.3. Forecasting Using ARMA and ARFIMA Models Here we present how we can use ARMA and ARFIMA models in one-step ahead time series forecasting. The specific applications of the fore casting methods are presented in Section 2.1.8. We consider a time series of n observations. Let us also consider a model fitted to the n observations and subsequently used to forecast x\n",
      "n+1. Let x n and ψ represent the last observation and the forecast of x n+1, respectively. The methods using ARMA and ARFIMA  models can be used as benchmarks in the simulation experiments. In fact, these methods ar e expected to perform better than the rest, when applied to the synthetic time series, since the la tter are simulated using ARMA or ARFIMA models (see Section 2.1.8). We examine two cases. In the first case we assume that the time series are modelled by an ARMA( p, q), in which p and q are predefined, while the parameters must be esti mated. We fit ARMA models to the data using the arima built in R function. During the implementation process, the values of p, q are set equal to the respective values used in the corresponding si mulation experiment. The function applies the maximum likelihood method to estimate th e values of the AR and MA parameters φ\n",
      "1, ..., φ p, θ1, ..., θ q of the models. We use the fitted ARMA model in fore cast mode by implementing the predict built in R function [52]. In the second case, we assume that the time series can be modelled by an ARFΙMA(p, d, q) model with unknown parameters. We fit an ARFIMA model to the data using the arfima function of the R package forecast [55,56]. The values of p, d, q are estimated using the Akaike Information Criterion with a correction for finite sample sizes (AICc), with d ranging between −0.5 and 0.5. The function applies the maximum likelihood method to estimate the values of the AR and MA parameters φ\n",
      "1, ..., φ\n",
      "p, θ1, ..., θ q of the models. The order selection and para meter estimation procedures are explained, Algorithms 2017, 10, 114  5 of 26 \n",
      "for example, in [57] (Chapter 8.6). We use the fitted ARFIMA model in forecast mode by implementing the forecast function of the forecast R package. 2.1.4. Forecasting Using Naïve Methods We use two naïve forecasting methods, the last observation naïve and the average naïve, which are amongst the most commonly used benchmarks [5 7] (Chapter 2.3). The produced forecasts are given by (16) and (17) respectively: ψ = x\n",
      "n (16)ψ = (x\n",
      "1 + … + x n)/n (17) The naïve methods presented here are benchmarks, because they are the simplest forecasting methods in terms of theoretical complexity and computational cost. 2.1.5. Forecasting Using the Theta Method We implement a simple version of the theta forecasting method, which was introduced in [49], through the thetaf function of the R package forecast . Theta uses a given time series to create two or more auxiliary time series with different (modified) local curvatures with respect to the original, namely the “Theta-lines”. The latter are extrapolat ed separately and subsequently combined with equal weights to produce the forecast. The modificati on of the local curvatures is implemented using the “Theta-coefficient” θ, which is applied to the second differences of the time series. The here adopted version uses two Theta-lines, for θ = 0 and θ = 2. This specific version performed well in the M3-Competition [37], whil e later [58] proved its equivalence to simple exponential smoothing (SES) with drift. For the exponential smoothing forecasting methods, and particularly for their implementation into a state space framework, the reader is referred to [57] (Chapter 7) and [59], respectively. 2.1.6. Random Forests RF is a vague term [20]. Here we use the original RF algorithm introduced in [18]. In particular, we present the version of the algorithm used for re gression. The specific version of the algorithm has been implemented in the randomForest R package [60]. The remainder of the section is largely reproduced from [20] with adaptations, while identica l notation is used in [20,60]. The present Section is not intended to provide a detailed description of RF. The interested reader can find further details in [20], while the computations are reproducible (Appendix A). We assume that the u is a random vector with k elements. The aim is to predict v by estimating \n",
      "the regression function: m(u) = E[v|u = u] \n",
      "(18) given the fitting sample: S\n",
      "s = ((u 1, v1), …, (u s, vs)) (19) which are independent realizations of the random variable ( u, v). Therefore, the aim is to construct \n",
      "an estimate m s of the function m. “A random forest is a predictor constructed by growing M randomized regression trees. For the j-th tree in the family, the predicted value at u is denoted by m\n",
      "s(u; θ j, S s), where θ 1, ..., θ M are \n",
      "independent random variables, distributed as θ and independent of S s. The random variable θ is used \n",
      "to resample the fitting set prior to the growing of  individual trees and to select the successive directions for splitting. The prediction is then given by the average of the predicted values of all trees. Before constructing each tree, b\n",
      "s observations are randomly chosen from the elements of u. These observations are used for growing the tree. At ea ch cell of the tree, a split is performed by maximization of the CART-criterion (defined in [20]) by selecting mtry variables randomly among the k original ones, picking the best variable/split point among the mtry and splitting the node into Algorithms 2017, 10, 114  6 of 26 \n",
      "two daughter nodes. The growing of the tree is sto pped when each cell contains fewer than nodesize points. The parameters used in RF are b\n",
      "s ∊ {1, …, s}, mtry ∊ {1, …, k}, nodesize ∊ {1, …, b s}, and M ∊ {1, 2, …}. In most studies, it is agreed that increasing the number of trees does not decrease the predictive performance; however, it results in an increase of the computational cost. Oshiro et al. [32] suggests a range between 64 and 128 trees in a forest based on experiments. Kuhn and Johnson [34] (p. 200) suggest using at least 1000 trees. Probst and Bouleste ix [33] suggest that the highest performance gain is achieved when training 100 trees. In the present study, we use M = 500 trees. The number of sampled data points in each tree and the number of examples in each cell below which the cell is not split are set equal to their de fault values in the randomForest R package, i.e., equal to the sample size and nodesize = 5, respecti vely. These are reported to be good values [20,35]. The parameter mtry is estimated during the valid ation phase by implementing the trainControl function of the R package caret [50,51]. The optimal mtry is found by using bootstrap resampling, i.e., randomly sampling the validation test with reselection (the size of the bootstrap sample must be equal to the size of the validation test), fitting the model in the bootstrap sample and measuring its performance in the remaining set, i.e., the data no t selected by the bootstrap sampling. Twenty-five iterations are used in the resampling procedure. Th e optimal mtry is found by calculating the average performance of the fitted models, while the sear ch for the optimal value of the parameter is performed in a grid. The root mean squared error (RMSE) is used to measure the performance. The optimal mtry minimizes the RMSE. Details on the th eoretical background of the method can be found in [34] (pp. 72–73). The regression function of Equation (18) is defined by the model, which is fitted to the validation test using the optimal mtry value. 2.1.7. Time Series Forecasting Using Random Forests Using RF for one-step time series forecasting is  straightforward and similar to the way that RF can be used for regression. Let g be the function obtained from the training of RF, which will be used for forecasting x\n",
      "n+1, given x 1, …, x n. If we use k lagged variables then the forecasted x n+1 is given by the following equation for t = n + 1: x\n",
      "t = g(x t−1, …, x t−k), t = k + 1, …, n + 1 (20) The function g is not in closed form, but can be obtained by training the RF algorithm using a training set of size n − k. In each sample of the fitting set the dependent variable is x\n",
      "t, for t = k + 1, …, n + 1, while the predictor variables are x\n",
      "t−1, …, x t−k. When the number of predictor variables k increases, the size of the training set n − k decreases (an example is presented in Figure 1). The training set, which includes n – k samples, is created using the CasesSeries function of the rminer R package [61,62]. Finally, the fitting is performed using the train function of the caret R package and the forecasted value of x\n",
      "n+1 is obtained using the predict function of the caret R package. \n",
      " \n",
      "Figure 1. Sketch explaining how the training sample changes with the number of predictor variables for time series with n = 5 and (a) k = 1, (b) k = 2. \n",
      "Algorithms 2017, 10, 114  7 of 26 \n",
      "Kuhn and Johnson [34] (p. 463) defines the variable importance as the overall quantification of the relationship between the predictor variables and the predicted value. Here we use the first measure of variable importance in RF as defined in the randomForest R package [60]. From the training procedure using k predictor variables, we keep the variables with positive variable importance and define a new set of predictor variables, which includes these variables. Then we repeat the forecasting procedure. In this case, the size of the sample depends on the minimum index of the subset of predictor variables when forecasting x\n",
      "n+1, e.g., if the minimum index is n + 1 − k, then the training set is of size n − k. However, using fewer predictor variables decreases the computational cost. 2.1.8. Summary of the Methods In Table 1, we summarize the methods presented in  Sections 2.1.3–2.1.7. In Table 2, we present the category of data to which we apply each specific case of the ARMA and ARFIMA model based methods in Section 3. In Table 3, we present the 12 different variable selection procedures used by the RF algorithm. \n",
      "Table 1. Summary of the methods presented in Sections 2.1.3–2.1.7 and their abbreviation as used in Section 3. Method Section Brief Explanation \n",
      "arfima 2.1.3 Uses fitted ARMA or ARFIMA models \n",
      "naïve1 2.1.4 Forecast equal to the last observed value, Equation (16) naïve2 2.1.4 Forecast equal to the mean of the fitted set, Equation (17) theta 2.1.5 Uses the theta method rf 2.1.7 Uses random forests, Equation (20) \n",
      "Table 2. Methods presented in Section 2.1.3 for fore casting using ARMA and ARFIMA models and their specific applications in Section 3. Method Application in Section 3 \n",
      "1st case in Section 2.1.3 Simulations from the family of ARMA models \n",
      "2nd case in Section 2.1.3 Simulations fr om the family of ARFIMA models, with d ≠ 0 2nd case in Section 2.1.3 Temperature data \n",
      "Table 3. Variable selection procedures used by the RF algorithm. Method Explanatory Variables \n",
      "rf05, rf10, rf15, rf20, rf25, rf30, rf35, rf40, rf45, rf50 Uses the last 5, …, 50 variables \n",
      "rf20imp, rf50imp Uses the most important variables from the last 20 and 50 variables respectively \n",
      "2.1.9. Metrics Here we define the metrics used in the comparisons. The error (E) is defined by: E := ψ − x\n",
      "n+1 (21) The absolute error (AE) is defined by: AE := |ψ − x\n",
      "n+1| (22) The squared error (SE) is defined by: SE := (ψ − x\n",
      "n+1)2 (23) The percentage error (PE) is defined by: Algorithms 2017, 10, 114  8 of 26 \n",
      "PE := (ψ − x n+1)/xn+1 (24) The absolute percentage error (APE) is defined by: APE := |ψ − x\n",
      "n+1/xn+1| (25) The metrics defined in Equations (21)–(25) can be  used for the assessment of the performance in single experiments. To assess the forecasting perf ormance using a set of multiple simulations, the metrics must be summarized. Let N be the number of the conducted forecasting experiments. Let the serial number of each experiment i be stated as a subscript to each metric value E\n",
      "i, AE i, SE i, PE i and APE\n",
      "i. Then, the mean of the errors (MoE) is defined by: MoE := (E\n",
      "1 + … + E N)/N (26) The median of the errors (MdoE) is defined by: MdoE := median(E\n",
      "1, …, E N) (27) The mean of the absolute errors (MoAE) is defined by: MoAE := (AE\n",
      "1 + … + AE N)/N (28) The median of the absolute errors (MdoAE) is defined by: MdoAE := median(AE\n",
      "1, …, AE N) (29) The mean of the squared errors (MoSE) is defined by: MoSE := (SE\n",
      "1 + … + SE N)/N (30) The median of the squared errors (MdoSE) is defined by: MdoSE := median(SE\n",
      "1, …, SE N) (31) The mean of the percentage errors (MoPE) is defined by: MoPE := (PE\n",
      "1 + … + PE N)/N (32) The median of the percentage errors (MdoPE) is defined by: MdoPE := median(PE\n",
      "1, …, PE N) (33) The mean of the absolute percentage errors (MoAPE) is defined by: MoAPE := (APE\n",
      "1 + … + APE N)/N (34) The median of the absolute percentage errors (MdoAPE) is defined by: MdoAPE := median(APE\n",
      "1, …, APE N) (35) The means and the medians can substantially differ  when outliers are observed; therefore, they are both useful in the assessment of the forecast ing performance. Another useful metric of the forecasting performance, when assessing multiple experiments simultaneously, is the slope of the regression. Let i also be stated as a subscript to each forecast ψ\n",
      "i and its corresponding true value x n+1,i. The regression coefficient a (or slope of the regression) is estimated to measure the dependence of ψ\n",
      "1, …, ψ\n",
      "N on x n+1,1, …, x n+1,N, when this dependence is expressed by the following linear regression model: ψ\n",
      "i = a x n+1,i + b (36) In Table 4, we present the range of the values of the metrics and their values when the forecast is perfect. \n",
      "  Algorithms 2017, 10, 114  9 of 26 \n",
      "Table 4. Metrics of forecasting performance, their rang e and respective values when the forecast is perfect. Metric Equation Range Metric Values for Perfect Forecast \n",
      "error (21) [−∞, ∞] 0 \n",
      "absolute error (22) [0, ∞] 0 squared error (23) [0, ∞] 0 percentage error (24) [−∞, ∞] 0 absolute percentage error (25) [0, ∞] 0 linear regression coefficient (36) [−∞, ∞] 1 \n",
      "A discussion on the appropriateness of the metric s to measure the performance of the forecasting methods can be found in [57] (Chapter 2.5) and [63], while [64,65] extensively discuss metrics connected to the regression coefficient. Here we pr eferred to use the metrics of Table 4, due to their simplicity. 2.2. Data Here we present the data used in the present study. We used two large datasets. The first was obtained from simulations from the models presented in Section 2.1.2. The second dataset is a set of observed annual temperatures. 2.2.1. Simulated Time Series We used 16 models from the family of ARFIMA models presented in Section 2.1.1. Each model corresponds to a simulation experiment as presented in Table 5. Each simulation experiment includes 1000 simulated time series of size 101 from the models of Table 5. \n",
      "Table 5. Simulation experiments, their respective models and their defined parameters. See Section 2.1.1 for the definitions of the parameters. Experiment Model Parameters \n",
      "1 ARMA(1, 0) φ 1 = 0.6 \n",
      "2 ARMA(1, 0) φ 1 = −0.6 3 ARMA(2, 0) φ\n",
      "1 = 0.6, φ 2 = 0.2 4 ARMA(2, 0) φ\n",
      "1 = −0.6, φ 2 = 0.2 5 ARMA(0, 1) θ\n",
      "1 = 0.6 6 ARMA(0, 1) θ\n",
      "1 = −0.6 7 ARMA(0, 2) θ\n",
      "1 = 0.6, θ 2 = 0.2 8 ARMA(0, 2) θ\n",
      "1 = −0.6, θ 2 = −0.2 9 ARMA(1, 1) φ\n",
      "1 = 0.6, θ 1 = 0.6 10 ARMA(1, 1) φ\n",
      "1 = −0.6, θ 1 = −0.6 11 ARMA(2, 2) φ\n",
      "1 = 0.6, φ 2 = 0.2, θ 1 = 0.6, θ 2 = 0.2 12 ARFIMA(0, 0.40, 0)  13 ARFIMA(1, 0.40, 0) φ\n",
      "1 = 0.6 14 ARFIMA(0, 0.40, 1) θ\n",
      "1 = 0.6 15 ARFIMA(1, 0.40, 1) φ\n",
      "1 = 0.6, θ 1 = 0.6 16 ARFIMA(2, 0.40, 2) φ\n",
      "1 = 0.6, φ 2 = 0.2, θ 1 = 0.6, θ 2 = 0.2 \n",
      "2.2.2. Temperature Dataset The real-world dataset includes 135 mean annual instrumental temperature time series extracted from the database of [48]. The database includes  observed mean monthly temperatures. We extracted Algorithms 2017, 10, 114  10 of 26 \n",
      "stations, which included mean monthly temperatur es for the period 1916–2016, i.e., 101 years of observations. We discarded stations with missing data. We depict the 135 remaining stations in Figure 2. The stations cover a considerable part of  the Earth’s surface, therefore, they can represent diverse behaviours observed in geophysical time series. The mean annual temperatures were obtained by averaging per year the monthly values. \n",
      " \n",
      "Figure 2. Map of locations for the 135 stations used in the analysis. \n",
      "3. Results Here we present the results from the application of the methods to the data presented in Section 2. We use two datasets including time series of si ze 101. The two datasets include simulated time series and observed temperatures. The fitting is perf ormed in the first 100 values of the time series, while the methods are compared in forecasting the 101st value. The application of the methods to specific datasets was presented in Section 2.1.8. 3.1. Simulations We present the results from the application of th e methods to the simulated time series. Each simulation experiment lasts 30 hours, while the computation time for all the methods excluding RF is some minutes. In particular, we present the boxplots of the absolute errors and the errors. Furthermore, we present the medians of the absolu te errors, medians of the squared errors and the regression coefficients. The rf methods were optimi zed by minimizing the RMSE, therefore, the most representative figures of their predictive perfor mance are the ones presenting the squared errors. Given that the information presented by the boxplots of the squared errors (available in the supplementary information) is minimal, we mostly pr esent here results related to the absolute errors of the forecasts. The two approaches yield equiva lent results, as shown in the present sections. Additionally, the primary aim of the present study is the optimization of the predictive performance of the RF, therefore, the results of the methods described in Section 2, should be assessed using various methods. Detailed results for all the experiments can be  found in the analysis.html file in Appendix A. In Figure 3, we present a typical example in whic h we test the methods when applied to the 1000 time series simulated from the ARMA(1, 0) model with φ\n",
      "1 = 0.6. In this typical example, we observe that all methods perform similarly with respect to the absolute and squared errors. The rf30 method is the best amongst the rf methods, while arfima has the best performance and naïve2 has the worst performance. The arfima method is approximately 5% better than the best rf methods. The naïve1 and theta methods perform similarly to the best rf methods. The simplest rf method, i.e., rf1, performs well. In fact, its performance is comparable to the best rf methods, i.e., rf20imp, rf25, and rf30. Regarding the use of important variables, introd uced with the rf20imp and the rf50imp methods, their performance is similar to that of the rf20 and rf50 methods, respectively. \n",
      "Algorithms 2017, 10, 114  11 of 26 \n",
      "When looking at the regression coefficients, the pattern of the performance of the methods is clear. The rf methods are better when using fewer predictor variables. The naïve1 method performs better than all methods, followed by the theta and arfima methods. \n",
      "Figure 3. Barplots of the medians of the absolute errors , medians of the squared errors and regression coefficients when forecasting the 101st value of 1000 simulated time series from an ARMA(1, 0) model with φ\n",
      "1 = 0.6. \n",
      "In Figures 4–6, we present the boxplots of the absolute errors for the cases of the ARMA and ARFIMA models. It seems that in most cases, the rf methods with less predictor variables are better compared to the rf methods with many predictor variables according to the median of the absolute errors. The range of the absolute errors seems to get wider when using more predictor variables, as well as the number and the magnitude of outliers. Th e differences between all cases of the rf are small. The arfima method is the best, as expected, while the naïve and theta methods have varying predictive performance. \n",
      " \n",
      "Algorithms 2017, 10, 114  12 of 26 \n",
      " \n",
      " \n",
      " \n",
      "Figure 4. Boxplots of the absolute errors when foreca sting the 101st value of 1000 simulated time series from each ARMA(p, 0) and ARMA(0, q) model used in the present study. \n",
      "Algorithms 2017, 10, 114  13 of 26 \n",
      " \n",
      "Figure 5. Boxplots of the absolute errors when foreca sting the 101st value of 1000 simulated time series from each ARMA(p, q) model used in the present study. \n",
      " \n",
      "Figure 6. Boxplots of the absolute errors when foreca sting the 101st value of 1000 simulated time series from each ARFIMA model used in the present study. \n",
      "Algorithms 2017, 10, 114  14 of 26 \n",
      "The distribution of the forecasts errors is also of interest. In Figures 7–9, we present the boxplots of the errors for the ARMA and ARFIMA cases. Al l the methods are unbiased, in the sense that the median forecast error is approximately equal to 0, while most boxplots are symmetric around 0. The dispersion of the errors presented in Figures 7–9 ha s already been quantified by the boxplots of the absolute errors in Figures 4–6. \n",
      " \n",
      " \n",
      " \n",
      "Figure 7. Boxplots of the errors when forecasting the 101st value of 1000 simulated time series from each ARMA(p, 0) and ARMA(0, q) model used in the present study. \n",
      "Algorithms 2017, 10, 114  15 of 26 \n",
      " \n",
      "Figure 8. Boxplots of the errors when forecasting the 101st value of 1000 simulated time series from each ARMA(p, q) model used in the present study. \n",
      "Algorithms 2017, 10, 114  16 of 26 \n",
      " \n",
      "Figure 9. Boxplots of the errors when forecasting the 101st value of 1000 simulated time series from each ARFIMA model used in the present study. \n",
      "Given the vast amount of experiments and results, as well as the small differences between the performances of the various rf methods, we summar ize the results using the rankings of the methods in each experiment, e.g., in Figure 10, we presen t the rankings of all methods in the simulation experiments with respect to the mean and median of  the absolute errors. Due to the small differences between all the rf methods, a heatmap presenting the computed values for the mean and median of the absolute errors (and not their rankings) would be almost monochrome and, thus, not appropriate for the purpose of the present study. \n",
      "Figure 10. Ranking of methods within each simulation experiment based on the mean ( top) and median (bottom) of the absolute errors. Better methods are presented with lower ranking value and blue colours. \n",
      "Algorithms 2017, 10, 114  17 of 26 \n",
      "Regarding the means of the absolute errors, the ar fima method exhibits the best performance in all cases. In most cases of the ARMA simulation  experiments (experiments numbered from one to 11), we observe that the rf methods are better than  the rest methods. We observe a pattern in which the performance of the rf methods decreases when the number of predictor variables increases. The rf20imp method is generally better than the rf20 me thod, while the rf50imp method is worse than the rf50 method. The naïve and theta methods perform simi larly. In the case of the ARFIMA simulations, the results are similar regarding the performance of the arfima methods and the intercomparison between the rf methods. However, in this case, th e naïve1 and theta methods perform better than the rf methods. When looking at the medians of the absolute erro rs, the arfima method has the best performance. In contrast to the cases of the means of the absolu te errors, the pattern of the decreasing performance of the rf methods, when increasing the number of  predictor variables is less uniform. Nevertheless, the overall pattern suggests that the performance of the rf decreases with the increase of the number of predictor variables. Importantly, Figure 10 highlights the competence of the rf methods in time series forecasting of small time series. In fact, rf  exhibit better performance in comparison to all the methods but the arfima benchmark. This is definitely worth consideration.  Moreover, in Figure 11, we present the ranking of the methods with respect to the mean and median of the squared errors. The pattern is similar to that of Figure 10. Indeed the increase in the number of predictor variables decreases the performance of the RF algorithm. \n",
      "Figure 11. Ranking of methods in each simulation experiment based on the mean ( top) and median (bottom) of the squared errors. Better methods are pr esented with lower ranking value and blue colours. \n",
      "In Figure 12, we present the ranking of the method s with respect to the regression coefficient. In general, the arfima methods are the best performi ng, while naïve2 is the worst method. We observe a pattern regarding the ARMA-simulated time series  in which the naïve1 and theta methods are the best performing in the simulated experiments 1, 3, 5, 7, 9, and 11. In these experiments, the parameter \n",
      "Algorithms 2017, 10, 114  18 of 26 \n",
      "values of the ARMA models are positive. In contrast , the simulation experiments 2, 4, 6, 8, and 10 correspond to ARMA models with at least one negative parameter. \n",
      " \n",
      "Figure 12. Ranking of methods in each simulation expe riment based on the regression coefficients. Better methods are presented with lowe r ranking value and blue colours. \n",
      "Regarding the rf methods, they are almost uniformly better when using fewer predictor variables. Furthermore, the methods using important variables are better, compared to the respective methods, in which the full number of predictor variables is used. 3.2. Temperature Analysis Regarding the application of the methods to the te mperature dataset, we present the full results in the temperature_analysis.html file in Appendix A. Here we present some results necessary for the following discussion. In more detail, in Figure 13, we present the boxplots of the forecasts and their comparison to the test values. In terms of this  specific comparison, naïve1 exhibits the best performance, since the median and range of its fo recasts closely resemble the corresponding test values. The naïve2 method is the worst. The other methods are similar. \n",
      " \n",
      "Figure 13. Boxplot of forecasts of temperature for each method. \n",
      "To highlight possible differences between the rf methods, we further present some comparisons based on the metric values. In Figure 14, we present the boxplots of the error, absolute error, squared error, and absolute percentage error values. Th e errors are mostly negative. Additionally, the respective distributions of the rf methods are closer to each other than to the respective distributions of naïve1, naïve2, and theta, while the distribution of the errors of the arfima method is rather closer to those of the rf methods. The same applies to the distributions of the other metrics. The far outliers \n",
      "Algorithms 2017, 10, 114  19 of 26 \n",
      "produced in terms of absolute percentage error by  almost all the methods can be explained by the presence of temperature values, which are close to zero. \n",
      " \n",
      " \n",
      "Figure 14. Boxplots of the error, absolute error, square d error, and absolute percentage error values of the temperature forecasts for all the methods. \n",
      "Moreover, Figure 15 focuses on the average-case performance of the methods with respect to all the computed metrics (presented in Section 2.1.9). A comparison with respect to this performance can ease the ranking of the methods. With regards to  this particular ranking, the naïve1 and theta methods are clearly the best, while the naïve2 method is clearly the worst. The arfima method exhibits worse performance than naïve1 and theta, but better than the naïve2 method. \n",
      "Algorithms 2017, 10, 114  20 of 26 \n",
      "Figure 15. Barplots of the medians of various types of metrics measuring the error of the temperature forecasts for all the methods. The types of errors are depicted in the vertical axes. \n",
      "Regarding the rf methods, the best performance is  exhibited by the rf10 method in terms of the median of errors, median of absolute error, and median of squared errors. On the contrary, with respect to the median of percentage errors and the median of absolute percentage errors the best rf method is rf20imp. In general, the performance of the rf05, rf10, rf15, rf20, rf20imp methods are close in terms of the median values of the metrics. The same applies to the performance of all the remaining rf methods, which nevertheless are worse. The ar fima method has a similar performance with the worst rf methods. In Figure 16, we present the regression coeffici ents between the forecasted and the test values. Here the differences between the methods are negligible. The best method is theta, followed by the rf05 and arfima methods. Moreover, the regression  coefficient increases with the increase of the number of predictor variables. Additionally, using the important variables results in better results, e.g., rf20imp is better than rf20. \n",
      "Algorithms 2017, 10, 114  21 of 26 \n",
      " \n",
      "Figure 16. Barplots of the regression coefficient of the linear model between the forecasted and the test values of the temperature dataset. \n",
      "4. Discussion Most of the studies focusing on time series fo recasting aim at proposing a forecasting method based on its performance (compared to other methods, usually a few), when applied within a small number of case studies (e.g., [8,14,26,28,30]). Recognizing this specific fact and aiming at providing a tangible contribution in time series forecasting using RF, we have conducted an extensive set of computational experiments. These experiments were designed to create an empirical background on the effect of the variable selection on the foreca sting performance, when implementing RF in one- step forecasting of short time series. We have ex amined two large datasets, a dataset composed of simulated time series and a temperature dataset. Th e latter is used complementary to the former for more concise findings. The provided outcome is clear for the selected procedure of parameter tuning and for both the datasets examined and, thus, we would like to suggest its consideration in future applications. RF have performed better mostly when using a few recently lagged predictor variables. This outcome could be explained by the fact that increasing th e number of lagged variables inevitably leads to reducing the length of the training set and concom itantly to reducing the information exploited from the original time series during the model fitting  phase. On the contrary using many predictor variables in other types of regression problems do es not seriously impact the predictive performance of RF [34] (p. 489). In general, the results suggest that the RF algori thm performs well in one-step ahead forecasting of short time series. Nevertheless, other forecast ing algorithms may exhibit better performance in some cases. This does not apply to every experiment  conducted, neither does it imply that RF should not be used in one-step ahead time series forecastin g in favour of these other algorithms as shown in  [24–26,30]. In fact, in [12] the interested reader can find a small-scale comparison on 50 geophysical time series, which illustrates the fact that an al gorithm can perform better or worse depending on the time series examined. Furthermore, we focused on the RF algorithm an d tried to improve its performance, under the condition that this algorithm was found to be comp etent. Therefore, there is a question of whether the condition that the random forests algorithm is competent is true, which will be answered in the following. The differences between the forecasting methods are mostly small and the performance of the RF algorithm is always comparable with the performance of other algorithms (this is the reason we used the ranking of the methods in the heatma ps and not the values of the metrics). The RF algorithm was found to be competent in the expe riments using the simulated dataset, being worse than the arfima algorithm (a good benchmark for th e case of the simulated time series) only, but in general better compared to the other methods (naïve 1, naïve2, and theta methods). The RF algorithm was found to be competent in the experiment usin g the temperature dataset, being better than the arfima and naïve2 methods and worse than naïve1 an d theta. We also found that the suboptimal RF (i.e., the ones that use many predictor variables) were mostly worse than the other methods, while \n",
      "Algorithms 2017, 10, 114  22 of 26 \n",
      "the optimized RF were competent, thus highlighting  the role of the variable selection in improving the performance of the RF. Based on this fact, we would like to propose the us e of a variety of algorithms in each forecasting application. Furthermore, since we believe that a ll the methodological choices (e.g., the variable selection in forecasting using machine learning) should be based on theory, or at least on evidence (which can be provided by extensive studies, like the present), rather than be made at will, we suggest that the performance of all the algorithms to be used in a specific application should have been previously optimized. In addition to the main contribution of the pres ent study, some other noteworthy findings have been derived through the analyses as well. These findings are summarized subsequently and highlight the merits of the adopted methodological framework. In general, we observe rather small differences between the various RF methods wh ich, however, might be essential to some applications. Importantly, we observe that simple  methods can perform extremely well with respect to the rest in specific autocorrelation schemes, as  also concluded in other large comparisons (e.g., [37]) or reported by evidence in smaller studies (e.g., [12]). Particularly for the experiment conducted on real-world time series, we observe that the mi nimum absolute error in forecasting next year’s temperature using RF was approximately equal to 0. 6 °C. This outcome is definitely important for geosciences, suggesting that temperature is a process difficult to forecast. Moreover, the present study is one of the first applying benchmarking to assess the forecasting performance of random forests in time series forecasting and, therefore, a discussion about its effectiveness is meaningful. The achievement of a long-term optimal performance of a specific machine learning algorithm requires a firm strategy, which could result through studies like the present. Benchmarking was proven useful in high lighting the small magnitude of the differences between the various RF methods, while providing an upper and a lower bound in the performance of the latter, thus enabling one of the main conclusi ons, i.e., that RF is competent in one-step ahead forecasting of small time series. Therefore, we acknowledge the benefits of the benchmarking implementation within large-scale comparisons for cr eating a more faithful image of the goodness of performance characterizing the forecasting algorithms that can hardly be examined analytically, e.g., other machine learning algorithms. Finally, reproducibility was a key consideration of this paper. The codes are available in the supplementary information, while the reader is en couraged to use them either for verification purposes, or for future research activities. 5. Conclusions Random forests are amongst the most popular machine learning algorithms. The improvement of their performance in one-step forecasting of shor t time series was certainly worth attempting, since they have also proven to be a competent foreca sting algorithm. These performance improvements and competencies are suggested by an extensive se t of experiments, which use large datasets in conjunction with performance benchmarking. The resu lts of these experiments clearly indicate that using a few recent variables as predictors during the fitting process leads to higher predictive accuracy for the random forest algorithm used in this study. Of course, there are limitations in our study. In particular, the results were obtained using short time series, while there may be better procedures to  find the optimal sets of parameters. However, this is the largest experiment conducted with the aforementioned aim. We hope that this outcome will be of use in future applications. \n",
      "Supplementary Materials: The supplementary material is available online at www.mdpi.com/1999- 4893/10/4/114/s1. Acknowledgments: We thank three anonymous reviewers and the Academic Editor whose suggestions and comments helped in considerably improving the manuscript.  Algorithms 2017, 10, 114  23 of 26 \n",
      "Author Contributions: The two authors worked on the paper and the implementation of the methodologies using the R programming language equally and on all of it s aspects. However, Hristos Tyralis is the main writer of this paper and Georgia Papacharalampous conceived the idea. Conflicts of Interest: The authors declare no conflict of interest. \n",
      "Appendix A The supplementary materials found as a zip file  at http://dx.doi.org/10.17632/nr3z96jmbm.1 include all codes and data used in the manuscript. There is a readme file with detailed instructions on how to run each code and save the results. Two .html files, including the results and visualizations presented here, as well as more results which could not be included here for reasons of brevity, but enhance our finding,s can also be found in the supplementary material. The analyses and visualizations were performed in R Programming Language [52] by using the contributed R packages caret [50,51], forecast [55,56], fracdiff [54], gdata [66], ggplot2 [67], randomForest [60], readr [68], reshape2 [69], and rminer [61,62]. References \n",
      "1. Shmueli, G. To explain or to predict? Stat. Sci. 2010, 25, 289–310, doi:10.1214/10-STS330. 2. Bontempi, G.; Taieb, S.B.; Le Borgne, Y.A. Machine le arning strategies for time series forecasting. In Business Intelligence (Lecture Notes in Business Information Processing); Aufaure, M.A., Zimányi, E., Eds.; Springer: Berlin/Heidelberg, Germany, 2013; Volume  138, pp. 62–77, doi:10.1007/978-3-642-36318-4_3. 3. De Gooijer, J.G.; Hyndman, R.J. 25 years of time series forecasting. Int. J. Forecast. 2006, 22, 443–473, doi:10.1016/j.ijforecast.2006.01.001. 4. Fildes, R.; Nikolopoulos, K.; Crone, S.F.; Syntetos, A.A. Forecasting and operational research: A review. J. Oper. Res. Soc. 2008, 59, 1150–1172, doi:10.1057/palgrave.jors.2602597. 5. Weron, R. Electricity price forecasting: A review of  the state-of-the-art with a look into the future. Int. J. Forecast. 2014, 30, 1030–1081, doi:10.1016/j.ijforecast.2014.08.008. 6. Hong, T.; Fan, S. Probabilistic electric load forecasting: A tutorial review. Int. J. Forecast. 2016, 32, 914–938, doi:10.1016/j.ijforecast.2015.11.011. 7. Taieb, S.B.; Bontempi, G.; Atiya, A.F.; Sorjamaa, A. A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition. Expert Syst. Appl. 2012, 39, 7067– 7083, doi:10.1016/j.eswa.2012.01.039. 8. Mei-Ying, Y.; Xiao-Dong, W. Chaotic time series prediction using least squares support vector machines. Chin. Phys. 2004, 13, 454–458, doi:10.1088/1009-1963/13/4/007. 9. Faraway, J.; Chatfield, C. Time series forecasting with neural networks: A comparative study using the air line data. J. R. Stat. Soc. C Appl. Stat. 1998, 47, 231–250, doi:10.1111/1467-9876.00109. 10. Yang, B.S.; Oh, M.S.; Tan, A.C.C. Machine condition prognosis based on regression trees and one-step- ahead prediction. Mech. Syst. Signal Process. 2008, \n",
      "22, 1179–1193, doi:10.1016/j.ymssp.2007.11.012. 11. Zou, H.; Yang, Y. Combining time series models for forecasting. Int. J. Forecast. 2004, 20, 69–84, doi:10.1016/S0169-2070(03)00004-9. 12. Papacharalampous, G.A.; Tyralis, H.; Koutsoyiannis,  D. Forecasting of geophysical processes using stochastic and machine learning algorithms. In Proc eedings of the 10th World Congress of EWRA on Water Resources and Environment “Panta Rh ei”, Athens, Greece, 5–9 July 2017. 13. Pérez-Rodríguez, J.V.; Torra, S.; Andrada-Félix, J. ST AR and ANN models: Forecasting performance on the Spanish “Ibex-35” stock index. J. Empir. Financ. 2005, 12, 490–509, doi:10.1016/j.jempfin.2004.03.001. 14. Khashei, M.; Bijari, M. A novel hybridization of arti ficial neural networks and ARIMA models for time series forecasting. Appl. Soft Comput. 2011, 11, 2664–2675, doi:10.1016/j.asoc.2010.10.015. 15. Yan, W. Toward automatic time-series forecasting using neural networks. IEEE Trans. Neural Netw. Lear. Stat. 2012, 23, 1028–1039, doi:10.1109/TNNLS.2012.2198074. 16. Babu, C.N.; Reddy, B.E. A moving-average filter based hybrid ARIMA–ANN model for forecasting time series data. Appl. Soft Comput. 2014, 23, 27–38, doi:10.1016/j.asoc.2014.05.028. 17. Lin, L.; Wang, F.; Xie, X.; Zhong, S. Random forests-based extreme learning machine ensemble for multi-regime time series prediction. Expert Syst. Appl. 2017, 85, 164–176, doi:10.1016/j.eswa.2017.04.013. 18. Breiman, L. Random Forests. Mach. Learn. 2001, 45, 5–32, doi:10.1023/A:1010933404324. Algorithms 2017, 10, 114  24 of 26 \n",
      "19. Scornet, E.; Biau, G.; Vert, J.P. Consistency of random forests. Ann. Stat. 2015, 43, 1716–1741, doi:10.1214/15- AOS1321. 20. Biau, G.; Scornet, E. A random forest guided tour. Test 2016, 25, 197–227, doi:10.1007/s11749-016-0481-7. 21. Hastie, T.; Tibshirani, R.; Friedman, J. The Elements of Statistical Learning, 2nd ed.; Springer: New York, NY, USA, 2009; doi:10.1007/978-0-387-84858-7. 22. Verikas, A.; Gelzinis, A.; Bacauskiene, M. Mining data with random forests: A survey and results of new tests. Pattern Recognit. 2011, 44, 330–349, doi:10.1016/j.patcog.2010.08.011. 23. Herrera, M.; Torgo, L.; Izquierdo, J.; Pérez-García, R. Predictive models for forecasting hourly urban water demand. J. Hydrol. 2010, 387, 141–150, doi:10.1016/j.jhydrol.2010.04.005. 24. Dudek, G. Short-term load forecasting using random forests. In Proceedings of the 7th IEEE International Conference Intelligent Systems IS’2014 (Advances in Intelligent Systems and Computing), Warsaw, Poland, 24–26 September 2014; Filev, D., Jabłkowski, J., Kacprzyk, J., Krawczak, M., Popchev, I., Rutkowski, L., Sgurev, V., Sotirova, E., Szynkarczyk, P., Zadrozny, S., Eds.; Springer: Cham, Switzerland, 2015; Volume 323, pp. 821–828, doi:10.1007/978-3-319-11310-4_71. 25. Chen, J.; Li, M.; Wang, W. Statistical uncertainty estimation using random forests and its application to drought forecast. Math. Probl. Eng. 2012, 2012, 915053, doi:10.1155/2012/915053. 26. Naing, W.Y.N.; Htike, Z.Z. Forecasting of monthl y temperature variations using random forests. APRN J. Eng. Appl. Sci. 2015, 10, 10109–10112. 27. Nguyen, T.T.; Huu, Q.N.; Li, M.J. Forecasting time series water levels on Mekong river using machine learning models. In Proceedings of the 2015 Seventh International Conference on Knowledge and Systems Engineering (KSE), Ho Chi Minh City, Vietnam, 8–10 October 2015; doi:10.1109/KSE.2015.53. 28. Kumar, M.; Thenmozhi, M. Forecasting stock index mo vement: A comparison of support vector machines and random forest. In Indian Institute of Capital Markets 9th Capital Markets Conference Paper; Indian Institute \n",
      "of Capital Markets: Vashi, India, 2006; doi:10.2139/ssrn.876544. 29. Kumar, M.; Thenmozhi, M. Forecasting stock in dex returns using ARIMA-SVM, ARIMA-ANN, and ARIMA-random forest hybrid models. Int. J. Bank. Acc. Financ. 2014, 5, 284–308, doi:10.1504/IJBAAF.2014.064307. 30. Kane, M.J.; Price, N.; Scotch, M.; Rabinowitz, P. Co mparison of ARIMA and Random Forest time series models for prediction of avian influenza H5N1 outbreaks. BMC Bioinform. 2014, 15, doi:10.1186/1471-2105- 15-276. 31. Genuer, R.; Poggi, J.M.; Tuleau-Malot, C. Variable selection using random forests. Pattern Recognit. Lett. 2010, 31, 2225–2236, doi:10.1016/j.patrec.2010.03.014. 32. Oshiro, T.M.; Perez, P.S.; Baranauskas, J.A.  How many trees in a random forest? In Machine Learning and Data Mining in Pattern Recognition (Lecture Notes in Computer Science); Perner, P., Ed.; Springer: Berlin/Heidelberg,  Germany, 2012; pp. 154–168; doi:10.1007/978-3-642-31537-4. 33. Probst, P.; Boulesteix, A.L. To tune or not to tune the number of trees in random forest? arXiv 2017, arXiv:1705.05654v1. 34. Kuhn, M.; Johnson, K. Applied Predictive Modeling; Springer: New York, NY, USA, 2013; doi:10.1007/978-1- 4614-6849-3. 35. Díaz-Uriarte, R.; De Andres, S.A. Gene selection and cl assification of microarray data using random forest. BMC Bioinform. 2006, 7, doi:10.1186/1471-2105-7-3. 36. Makridakis, S.; Hibon, M. Confidence intervals: An  empirical investigation of the series in the M- competition. Int. J. Forecast. 1987, 3, 489–508, doi:10.1016/0169-2070(87)90045-8. 37. Makridakis, S.; Hibon, M. The M3-Competiti on: Results, conclusions and implications. Int. J. Forecast. 2000, 16, 451–476, doi:10.1016/S0169-2070(00)00057-1. 38. Pritzsche, U. Benchmarking of classical and machin e-learning algorithms (with special emphasis on bagging and boosting approaches) for time series forecasting. Master’s Thesis, Ludwig-Maximilians-Universität München, München, Germany, 2015. 39. Bagnall, A.; Cawley, G.C. On the use of default parameter settings in the empirical evaluation of classification algorithms. \n",
      "arXiv 2017, arXiv:1703.06777v1. \n",
      "  Algorithms 2017, 10, 114  25 of 26 \n",
      "40. Salles, R.; Assis, L.; Guedes, G.; Bezerra, E.; Porto,  F.; Ogasawara, E. A framework for benchmarking machine learning methods using linear models for univar iate time series prediction. In Proceedings of the 2017 International Joint Conference on  Neural Networks (IJCNN), Anchorage, AK, USA, 14–19 May 2017; 2338–2345, doi:10.1109/IJCNN.2017.7966139. 41. Bontempi, G. Machine Learning Strategies for Time Series Prediction. European Business Intelligence Summer School, Hammamet, Lecture. 2013. Availa ble online: https://pdfs.semanticscholar.org/ f8ad/a97c142b0a2b1bfe20d8317ef58527ee329a.pdf (accessed on 25 September 2017). 42. McShane, B.B. Machine Learning Methods with Time  Series Dependence. Ph.D. Thesis, University of Pennsylvania, Philadelphia, PA, USA, 2010. 43. Bagnall, A.; Bostrom, A.; Large, J.; Lines, J. Simulated data experiments for time series classification part 1: Accuracy comparison with default settings. arXiv 2017, arXiv:1703.09480v1. 44. Box, G.E.P.; Jenkins, G.M. Some recent advances in forecasting and control. J. R. Stat. Soc. C Appl. Stat. 1968, 17, 91–109, doi:10.2307/2985674. 45. Wei, W.W.S. Time Series Analysis, Univariate and Multivariate Methods , 2nd ed.; Pearson Addison Wesley: Boston, MA, USA, 2006; ISBN 0-321-322116-9. 46. Thissen, U.; Van Brakel, R.; De Weijer, A.P.; Mels sena, W.J.; Buydens, L.M.C. Using support vector machines for time series prediction. Chemom. Intell. Lab. 2003, 69, 35–49, doi:10.1016/S0169-7439(03)00111- 4. 47. Zhang, G.P. An investigation of neural networks for linear time-series forecasting. Comput. Oper. Res. 2001, 28, 1183–1202, doi:10.1016/S0305-0548(00)00033-2. 48. Lawrimore, J.H.; Menne, M.J.; Gleason, B.E.; Williams, C.N.; Wuertz, D.B.; Vose, R.S.; Rennie, J. An overview of the Global Historical Climatology Netw ork monthly mean temperature data set, version 3. J. Geophys. Res. 2011, 116, doi:10.1029/2011JD016187. 49. Assimakopoulos, V.; Nikolopoulos, K. The theta mo del: A decomposition approach to forecasting. Int. J. Forecast. 2000, 16, 521–530, doi:10.1016/S0169-2070(00)00066-2. 50. Kuhn, M. Building predictive models in R using the caret package. J. Stat. Softw. 2008, 28, doi:10.18637/jss.v028.i05. 51. Kuhn, M.; Wing, J.; Weston, S.; Williams, A.; Keefer, C .; Engelhardt, A.; Cooper, T.; Mayer, Z.; Kenkel, B.; \n",
      "The R Core Team; et al. Caret: Classification and Regression Training , R package version 6.0-76; 2017. 52. The R Core Team. R: A Language and Environment for Statistical Computing; R Foundation for Statistical Computing: Vienna, Austria, 2017. 53. Hemelrijk, J. Underlining random variables. Stat. Neerl. 1966, 20, 1–7. 10.1111/j.1467-9574.1966.tb00488.x. 54. Fraley, C.; Leisch, F.; Maechler, M.; Reisen, V.; Lemonte, A. Fracdiff: Fractionally Differenced ARIMA aka ARFIMA(p,d,q) Models, R package version 1.4-2; 2012. 55. Hyndman, R.J.; O’Hara-Wild, M; Bergmeir, C.; Razbash, S.; Wang, E. Forecast: Forecasting Functions for Time Series and Linear Models, R package version 8.1.; 2017. 56. Hyndman, R.J.; Khandakar, Y. Automatic time series forecasting: The forecast package for R. J. Stat. Softw. 2008, 27, doi:10.18637/jss.v027.i03. 57. Hyndman, R.J.; Athanasopoulos, G. Forecasting: Principles and Practice; OTexts: Melbourne, Australia, 2013. Available online: http://otexts.org/fpp/ (accessed on 25 September 2017). \n",
      "58. Hyndman, R.J.; Billah, B. Unmasking the Theta method. Int. J. Forecast. 2003, 19, 287–290, doi:10.1016/S0169- 2070(01)00143-1. 59. Hyndman, R.J.; Koehler, A.B.; Ord, J.K.; Snyder, R.D. Forecasting with Exponential Smoothing: The State Space Approach; Springer: Berlin/Heidelberg, Germany, 2008; doi:10.1007/978-3-540-71918-2. 60. Liaw, A.; Wiener, M. Classification and Regression by randomForest. R News 2002, 2, 18–22. 61. C or tez, P . Dat a m in in g  with  n eu r al n etwor k s an d su ppor t ve ctor  m ach i n es u sin g  th e  R/r m in er  tool. In  Advances in Data Mining. Applications and Theoretical  Aspects (Lecture Notes in Artificial Intelligence); Perner, P., Ed.; Springer: Berlin/Heidelberg, Germany,  2010; Volume 6171, pp. 572–583, doi:10.1007/978-3-642- 14400-4_44. 62. Cortez, P. Rminer: Data Mining Classification and Regression Methods , R package version 1.4.2; 2016. 63. Hyndman, R.J.; Koehler, A.B. Another l ook at measures of forecast accuracy. Int. J. Forecast. 2006, 22, 679– 688, doi:10.1016/j.ijforecast.2006.03.001. Algorithms 2017, 10, 114  26 of 26 \n",
      "64. Alexander, D.L.J.; Tropsha, A.; Winkler, D.A. Beware of R2: Simple, unambiguous assessment of the prediction accuracy of QSAR and QSPR models. J. Chem. Inf. Model. 2015, 55, 1316–1322, doi:10.1021/acs.jcim.5b00206. 65. Gramatica, P.; Sangion, A. A historical excursus on the statistical validation parameters for QSAR models: A clarification concerning metrics and terminology.  J. Chem. Inf. Model. 2016, 56, 1127–1131, doi:10.1021/acs.jcim.6b00088. 66. Warnes, G.R.; Bolker, B.; Gorjanc, G.; Grothendie ck, G.; Korosec, A.; Lumley, T.; MacQueen, D.; Magnusson, A.; Rogers, J. Gdata: Various R Programming Tools for Data Manipulation , R package version 2.18.0; 2017. \n",
      "67. Wickham, H. Ggplot2: Elegant Graphics for Data Analysis , 2nd ed.; Springer International Publishing: Cham, Switzerland, 2016; doi:10.1007/978-3-319-24277-4. 68. Wickham, H.; Hester, J.; Francois, R.; Jylänki, J.; Jørgensen, M. Readr: Read Rectangular Text Data, R package version 1.1.1; 2017. 69. Wickham, H. Reshaping data with the reshape package. J. Stat. Softw. 2007, 21, doi:10.18637/jss.v021.i12. © 2017 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and condit ions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/). \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024bd01f",
   "metadata": {},
   "source": [
    "### Rotating the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "432099e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfWrite = PdfFileWriter()\n",
    "page_end = pdfReader.getPage(number_of_pages - 1).rotateClockwise(90)\n",
    "pdfWrite.addPage(page_end)\n",
    "\n",
    "with open(r'paper1_rotate_page.pdf', 'wb') as fh:\n",
    "    pdfWrite.write(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78fccc",
   "metadata": {},
   "source": [
    "### Merging PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db7120bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfFileMerger\n",
    "merger = PdfFileMerger()\n",
    "file1 = 'paper1.pdf'\n",
    "file2 = 'paper1_rotate_page.pdf'\n",
    "pdf_files = [file1,file2]\n",
    "\n",
    "for file in pdf_files:\n",
    "    merger.append(file)\n",
    "\n",
    "merger.write('merged_file.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30cd1b",
   "metadata": {},
   "source": [
    "### Splitting the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3620dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: paper1.pdf_page_1.pdf\n",
      "Created: paper1.pdf_page_2.pdf\n",
      "Created: paper1.pdf_page_3.pdf\n",
      "Created: paper1.pdf_page_4.pdf\n",
      "Created: paper1.pdf_page_5.pdf\n",
      "Created: paper1.pdf_page_6.pdf\n",
      "Created: paper1.pdf_page_7.pdf\n",
      "Created: paper1.pdf_page_8.pdf\n",
      "Created: paper1.pdf_page_9.pdf\n",
      "Created: paper1.pdf_page_10.pdf\n",
      "Created: paper1.pdf_page_11.pdf\n",
      "Created: paper1.pdf_page_12.pdf\n",
      "Created: paper1.pdf_page_13.pdf\n",
      "Created: paper1.pdf_page_14.pdf\n",
      "Created: paper1.pdf_page_15.pdf\n",
      "Created: paper1.pdf_page_16.pdf\n",
      "Created: paper1.pdf_page_17.pdf\n",
      "Created: paper1.pdf_page_18.pdf\n",
      "Created: paper1.pdf_page_19.pdf\n",
      "Created: paper1.pdf_page_20.pdf\n",
      "Created: paper1.pdf_page_21.pdf\n",
      "Created: paper1.pdf_page_22.pdf\n",
      "Created: paper1.pdf_page_23.pdf\n",
      "Created: paper1.pdf_page_24.pdf\n",
      "Created: paper1.pdf_page_25.pdf\n",
      "Created: paper1.pdf_page_26.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#fname = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "pdf = PdfFileReader('paper1.pdf')\n",
    "\n",
    "for page in range(pdf.getNumPages()):\n",
    "    pdfwrite = PdfFileWriter()\n",
    "    pdfwrite.addPage(pdf.getPage(page))\n",
    "    outputfilename = '{}_page_{}.pdf'.format(\n",
    "        filename, page+1)\n",
    "    with open(outputfilename, 'wb') as out:\n",
    "        pdfwrite.write(out)\n",
    "    print('Created: {}'.format(outputfilename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9d844",
   "metadata": {},
   "source": [
    "### Encrypting a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65e162ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    }
   ],
   "source": [
    "pdf_read = PyPDF2.PdfFileReader('paper1.pdf')\n",
    "pdf_write = PyPDF2.PdfFileWriter()\n",
    "for page in range(pdf_read.getNumPages()):\n",
    "    pdf_write.addPage(pdf_read.getPage(page))\n",
    "\n",
    "password = '123456'\n",
    "pdfwrite.encrypt(user_pwd=password, owner_pwd=None,use_128bit=True)\n",
    "\n",
    "with open(\"outputpdf.pdf\", 'wb') as fh:\n",
    "    pdf_write.write(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d85f07",
   "metadata": {},
   "source": [
    "### Adding watermark \n",
    "\n",
    "- A watermark is an identifying image or pattern that appears on each page. It can be a company logo or any strong information to be reflected on each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# originalfile = r\"file.pdf\"\n",
    "# watermark = r\"watermark.pdf\"\n",
    "# watermarkedfile = r\"watermarkedfile.pdf\"\n",
    "# watermark = PdfFileReader(watermark)\n",
    "# watermarkpage = watermark.getPage(0)\n",
    "# pdf = PdfFileReader(originalfile)\n",
    "# pdfwrite = PdfFileWriter()\n",
    "# for page in range(pdf.getNumPages()):\n",
    "#     pdfpage = pdf.getPage(page)\n",
    "#     pdfpage.mergePage(watermarkpage)\n",
    "#     pdfwrite.addPage(pdfpage)\n",
    "# with open(watermarkedfile, 'wb') as fh:\n",
    "#     pdfwrite.write(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ee1da",
   "metadata": {},
   "source": [
    "### Extracting text: pytesseract vs PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262d42f7",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "306e3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6f087f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithms 2017, 10, 114 26 of 26\n",
      "\n",
      "64.\n",
      "\n",
      "65.\n",
      "\n",
      "66.\n",
      "\n",
      "67.\n",
      "\n",
      "68.\n",
      "\n",
      "69.\n",
      "\n",
      "Alexander, D.LJ.; Tropsha, A.; Winkler, D.A. Beware of R* Simple, unambiguous assessment of the\n",
      "prediction accuracy of QSAR and QSPR models. J. Chem. Inf. Model. 2015, 55, 1316-1322,\n",
      "doi:10.1021/acs.jcim.5b00206.\n",
      "\n",
      "Gramatica, P.; Sangion, A. A historical excursus on the statistical validation parameters for QSAR models:\n",
      "A clarification concerning metrics and terminology. J. Chem. Inf. Model. 2016, 56, 1127-1131,\n",
      "doi:10.1021/acs.jcim.6b00088.\n",
      "\n",
      "Warnes, G.R.; Bolker, B.; Gorjanc, G.; Grothendieck, G.; Korosec, A.; Lumley, T.; MacQueen, D.;\n",
      "Magnusson, A.; Rogers, J. Gdata: Various R Programming Tools for Data Manipulation, R package version\n",
      "2.18.0; 2017.\n",
      "\n",
      "Wickham, H. Ggplot2: Elegant Graphics for Data Analysis, 2nd ed.; Springer International Publishing: Cham,\n",
      "Switzerland, 2016; doi:10.1007/978-3-319-24277-4.\n",
      "\n",
      "Wickham, H.; Hester, ].; Francois, R.; Jylanki, J.; Jorgensen, M. Readr: Read Rectangular Text Data, R package\n",
      "version 1.1.1; 2017.\n",
      "\n",
      "Wickham, H. Reshaping data with the reshape package. . Stat. Softw. 2007, 21, doi:10.18637/jss.v021.i12.\n",
      "\n",
      "@ © 2017 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access\n",
      "@ article distributed under the terms and conditions of the Creative Commons Attribution\n",
      "%\n",
      "\n",
      "(CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('page_26.png')\n",
    "text = pytesseract.image_to_string(image, lang = 'eng')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "93be9bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithms 2017, 10, 114  26 of 26 \n",
      "64. Alexander, D.L.J.; Tropsha, A.; Winkler, D.A. Beware of R2: Simple, unambiguous assessment of the prediction accuracy of QSAR and QSPR models. J. Chem. Inf. Model. 2015, 55, 1316–1322, doi:10.1021/acs.jcim.5b00206. 65. Gramatica, P.; Sangion, A. A historical excursus on the statistical validation parameters for QSAR models: A clarification concerning metrics and terminology.  J. Chem. Inf. Model. 2016, 56, 1127–1131, doi:10.1021/acs.jcim.6b00088. 66. Warnes, G.R.; Bolker, B.; Gorjanc, G.; Grothendie ck, G.; Korosec, A.; Lumley, T.; MacQueen, D.; Magnusson, A.; Rogers, J. Gdata: Various R Programming Tools for Data Manipulation , R package version 2.18.0; 2017. \n",
      "67. Wickham, H. Ggplot2: Elegant Graphics for Data Analysis , 2nd ed.; Springer International Publishing: Cham, Switzerland, 2016; doi:10.1007/978-3-319-24277-4. 68. Wickham, H.; Hester, J.; Francois, R.; Jylänki, J.; Jørgensen, M. Readr: Read Rectangular Text Data, R package version 1.1.1; 2017. 69. Wickham, H. Reshaping data with the reshape package. J. Stat. Softw. 2007, 21, doi:10.18637/jss.v021.i12. © 2017 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and condit ions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/). \n"
     ]
    }
   ],
   "source": [
    "pdf_read = PyPDF2.PdfFileReader('page_26.pdf')\n",
    "text = \"\"\n",
    "for i in range(0, pdf_read.numPages):\n",
    "    pageObj = pdf_read.getPage(i)\n",
    "    text = text + pageObj.extractText()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76064a",
   "metadata": {},
   "source": [
    "Note: PyPDF2 outperforms pytesseract in terms of preserving the pattern and format of the original document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd3f8c",
   "metadata": {},
   "source": [
    "#### Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5e37ba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "装 修 合 同\n",
      "\n",
      "委 托 方 ( 甲 方 ) :\n",
      "承 接 方 ( 乙 方 ) :\n",
      "工 程 项 目 :\n",
      "\n",
      " \n",
      "\n",
      "甲 、 乙 双 方 经 过 协 商 , 甲 方 决 定 委 托 乙 方 迹 行 以 下 部 分 居 室 装 潢 :\n",
      "\n",
      "一 、 工 程 概 况\n",
      "1) 工 程 地 址 :\n",
      "2) 居 室 规 格 :\n",
      "3) 施 工 内 容 :\n",
      "(1) - 逄 见 附 似 1\n",
      "(2)\n",
      "(3)\n",
      "(4)\n",
      ") 工 程 开 工 日 期 : _ 年 _ 月\n",
      "5) 工 程 诉 工 日 期 , _ 年\n",
      "6) 工 程 总 天 数 : _ 天\n",
      "7) 工 程 施 工 中 , 如 有 项 目 增 减 或 需 要 变 动 , 双 方 应 签 订 补 充 合 同 。\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "目\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "二 、 工 程 价 款 及 结 算\n",
      "1) 工 程 总 价 款 : 元 , 大 写 ( 新 加 坡 币 ) :\n",
      "详 见 本 合 同 装 修 价 格 清 单 。\n",
      "2) 工 程 款 付 款 方 式 :\n",
      "(1) 第 一 期 : 对 预 算 设 计 认 可 , 签 订 合 同 之 日 支 付 工 程 总 造 价 的 50% , 即\n",
      "作 为 定 金 。\n",
      "(2) 第 二 期 : 完 成 主 体 工 程 , 支 付 总 造 价 的 40%, 即 _。\n",
      "(3) 第 三 期 : 竣 工 验 收 合 格 后 , 支 付 工 程 总 造 价 的 5%, 即 _。\n",
      "(4) 第 四 期 : 3 个 月 保 质 期 完 成 后 , 支 付 工 程 总 造 价 的 5%, 即 。\n",
      "\n",
      " \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "tessdata_dir_config = \"--tessdata-dir \\\"tessdata/\\\"\"\n",
    "image = Image.open(\"test3.png\")\n",
    "text = pytesseract.image_to_string(image, lang = \"chi_sim\", config = tessdata_dir_config)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c74dfc1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "装 修 合 同\n",
      "委 托 方 （ 甲 方 ） ：\n",
      "承 接 方 （ 乙 方 ） ：\n",
      "工 程 项 目 ：\n",
      "甲 、 乙 双 方 经 过 协 商 ， 甲 方 决 定 委 托 乙 方 进 行 以 下 部 分 居 室 装 潢 ：\n",
      "一 、 工 程 概 况\n",
      "1） 工 程 地 址 ：\n",
      "2） 居 室 规 格 ：\n",
      "3）施 工 内 容 ：\n",
      "（1）详 见 附 件 1\n",
      "（2）\n",
      "（3）\n",
      "（4）\n",
      "4） 工 程 开 工 日 期 ： _ _ _ _ _ _ _ 年_ _ _ _ _ _ _ 月_ _ _ _ _ _ _ _ 日\n",
      "5） 工 程 竣 工 日 期 ： _ _ _ _ _ _ _ _ _ _ 年_ _ _ _ _ _ _ _ _ 月_ _ _ _ _ _ _ _ _ 日\n",
      "6） 工 程 总 天 数 ： _ _ _ _ _ _ _ _ _ _ _ _ _ 天\n",
      "7） 工 程 施 工 中 ， 如 有 项 目 增 减 或 需 要 变 动 ， 双 方 应 签 订 补 充 合 同 。\n",
      "二 、 工 程 价 款 及 结 算\n",
      "1） 工 程 总 价 款 ： 元 ， 大 写 （ 新 加 坡 币 ） ： ，\n",
      "详 见 本 合 同 装 修 价 格 清 单 。\n",
      "2） 工 程 款 付 款 方 式 ：\n",
      "（1） 第 一 期 ： 对 预 算 设 计 认 可 ， 签 订 合 同 之 日 支 付 工 程 总 造 价 的 5 0 % ， 即\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ 作 为 定 金 。\n",
      "（2） 第 二 期 ： 完 成 主 体 工 程 ， 支 付 总 造 价 的 4 0 % ， 即 _ _ _ _ _ _ _ _ _ _ 。\n",
      "（3） 第 三 期 ： 竣 工 验 收 合 格 后 ， 支 付 工 程 总 造 价 的 5 % ， 即 _ _ _ _ _ _ _ _ _ _ 。\n",
      "（4） 第 四 期 ： 3个 月 保 质 期 完 成 后 ， 支 付 工 程 总 造 价 的 5 % ， 即 _ _ _ _ _ _ _ _ _ _ 。\n"
     ]
    }
   ],
   "source": [
    "pdf_read = PyPDF2.PdfFileReader('test3.pdf')\n",
    "text = \"\"\n",
    "for i in range(0, pdf_read.numPages):\n",
    "    \n",
    "    pageObj = pdf_read.getPage(i)\n",
    "    print(pageObj.extractText())\n",
    "    text = text + pageObj.extractText()\n",
    "\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e416b1f",
   "metadata": {},
   "source": [
    "Note: PyPDF2 outperforms pytesseract in terms of extracting Chinese text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82fdba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
